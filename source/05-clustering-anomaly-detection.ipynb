{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.stats import gamma, kstest\n",
    "import pandas as pd\n",
    "#import spacy\n",
    "#!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz\n",
    "!pip install vaderSentiment\n",
    "import vaderSentiment.vaderSentiment as vader\n",
    "from sklearn.cluster import KMeans\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from detail.altairdf import altairDF\n",
    "alt.renderers.enable(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet files used in this notebook were created from the raw Kaggle CSV as follows:\n",
    "```python\n",
    "with open(\"data/amazon-reviews.csv\") as f:\n",
    "    data = pd.read_csv(f)\n",
    "data = data.sample(10000).reset_index(drop=True)\n",
    "data = data.drop(columns=[\"Id\", \"ProductId\", \"UserId\", \"ProfileName\", \"Time\", \"Summary\"])\n",
    "data[\"hscore\"] = \\\n",
    "    data.apply(lambda row: (1+row[\"HelpfulnessNumerator\"]) / (2+row[\"HelpfulnessDenominator\"]), axis=1)\n",
    "data = data.drop(columns=[\"HelpfulnessNumerator\", \"HelpfulnessDenominator\"])\n",
    "data = data.rename(columns={\"Score\":\"score\", \"Text\":\"text\"})\n",
    "data = data[[\"score\", \"hscore\", \"text\"]]\n",
    "data.to_parquet(\"data/amazon-reviews-10K.parquet\", compression=\"brotli\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterdf(df, pred):\n",
    "    return df.loc[[idx for idx in df.index if pred(df.loc[idx])]]\n",
    "def showtxt(df, subset = [\"text\"]):\n",
    "    return df.style \\\n",
    "             .applymap(lambda x: 'white-space:wrap', subset=subset) \\\n",
    "             .applymap(lambda x:'text-align:left', subset=subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_parquet(\"data/amazon-reviews-50K.parquet\").reindex()\n",
    "showtxt(reviews.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english = spacy.load('en_core_web_sm')\n",
    "#def sentences(text):\n",
    "#    return [str(s) for s in english(text).sents]\n",
    "\n",
    "sdelim = re.compile('(?<=[.!?]) *')\n",
    "def sentences(text):\n",
    "    return [s for s in re.split(sdelim, text) if len(s) > 1]\n",
    "\n",
    "sentiment = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_compound(text):\n",
    "    scores = [sentiment.polarity_scores(s)['compound'] for s in sentences(text)]\n",
    "    if len(scores) < 1: return 0.0\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reviews[\"sentiment\"] = reviews[\"text\"].apply(sentiment_compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "feats1 = reviews.copy().reindex()\n",
    "feats1[\"feats\"] = feats1.apply(lambda row: np.array([row[\"score\"] / 5.0, row[\"sentiment\"]]), axis=1)\n",
    "feats1[\"feats\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1[\"x: score\"] = feats1[\"feats\"].apply(lambda x: x[0])\n",
    "feats1[\"y: sentiment\"] = feats1[\"feats\"].apply(lambda x: x[1])\n",
    "alt.Chart(feats1.sample(2000)).encode(x=\"x: score\", y=\"y: sentiment\", color=\"score\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = np.array(list(feats1[\"feats\"]))\n",
    "clustering = KMeans(n_clusters=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1[\"pred\"] = clustering.predict(np.array(list(feats1[\"feats\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1[\"pstr\"] = feats1[\"pred\"].apply(str)\n",
    "alt.Chart(feats1.sample(2000)).encode(x=\"x: score\", y=\"y: sentiment\", color=\"pstr\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1[\"pdist\"] = feats1.apply(lambda row: np.linalg.norm(row[\"feats\"] - clustering.cluster_centers_[row[\"pred\"]]), axis=1)\n",
    "feats1[\"pdist\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = feats1.sort_values(by=[\"pdist\"], ascending=False)[[\"pdist\",\"sentiment\",\"score\",\"text\"]].head(25)\n",
    "showtxt(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingles(k):\n",
    "    def kshingles(doc):\n",
    "        return [doc[i:i + k] for i in range(len(doc) - k + 1)]\n",
    "    return kshingles\n",
    "\n",
    "htmlbr = re.compile('<br />')\n",
    "whitesp = re.compile('\\\\s+')\n",
    "def cleantxt(txt):\n",
    "    clean = re.sub(htmlbr, ' ', txt)\n",
    "    clean = re.sub(whitesp, ' ', clean)\n",
    "    clean = clean.lower()\n",
    "    return clean\n",
    "\n",
    "def hashing_frequency(vecsize, h, norm = 1.0):\n",
    "    def hf(words):\n",
    "        if type(words) is type(\"\"):\n",
    "            # handle both lists of words and space-delimited strings\n",
    "            words = words.split(\" \")\n",
    "        hsig = np.zeros(vecsize, dtype=np.float32)\n",
    "        for term in [w for w in words if len(w) > 0]:\n",
    "            hsig[h(term) % vecsize] += 1.0\n",
    "        z = np.linalg.norm(hsig) / norm\n",
    "        if (z > 0.0): hsig /= z\n",
    "        return hsig\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sh4 = shingles(4)\n",
    "hsig = hashing_frequency(512, hash, norm = 1)\n",
    "feats2 = reviews.copy()\n",
    "feats2[\"feats\"] = feats2[\"text\"].apply(lambda txt: hsig(sh4(cleantxt(txt))))\n",
    "feats2[\"feats\"].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "def append_pca_columns(df, featcol, pcacols=[\"x\", \"y\"]):\n",
    "    DIMENSIONS = 2\n",
    "    data = np.array(list(df[featcol]))\n",
    "    pca2 = sklearn.decomposition.PCA(DIMENSIONS)\n",
    "    pca = pca2.fit_transform(data)\n",
    "    pca_df = pd.DataFrame(pca, columns=pcacols)\n",
    "    df = df.drop(columns=pcacols, errors='ignore')\n",
    "    df = pd.concat([df, pca_df], axis=1).reindex()\n",
    "    return df\n",
    "\n",
    "def pca_features(df, icol, ocol, dimensions=2):\n",
    "    data = np.array(list(df[icol]))\n",
    "    pca2 = sklearn.decomposition.PCA(dimensions)\n",
    "    pca = pca2.fit_transform(data)\n",
    "    df[ocol] = list(pca)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats2 = append_pca_columns(feats2, \"feats\")\n",
    "alt.Chart(feats2.sample(2000)).encode(x=\"x\", y=\"y\", color=\"score\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = np.array(list(feats2[\"feats\"]))\n",
    "clustering = KMeans(n_clusters=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats2[\"pred\"] = clustering.predict(np.array(list(feats2[\"feats\"])))\n",
    "feats2[\"pstr\"] = feats2[\"pred\"].apply(str)\n",
    "alt.Chart(feats2.sample(2000)).encode(x=\"x\", y=\"y\", color=\"pstr\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats2[\"pdist\"] = feats2.apply(lambda row: np.linalg.norm(row[\"feats\"] - clustering.cluster_centers_[row[\"pred\"]]), axis=1)\n",
    "feats2[\"pdist\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anomalies = feats2.sort_values(by=[\"pdist\"], ascending=False)[[\"pdist\",\"score\",\"sentiment\",\"text\"]].head(25)\n",
    "showtxt(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "\n",
    "HVSIZE = 1000\n",
    "vectorizer = HashingVectorizer(token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', n_features = HVSIZE, alternate_sign=False)\n",
    "hvcounts = vectorizer.fit_transform(reviews[\"text\"].apply(cleantxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normarray(v):\n",
    "    r = v.toarray().reshape(HVSIZE)\n",
    "    z = np.linalg.norm(r)\n",
    "    if (z > 0.0): r /= z\n",
    "    return r\n",
    "\n",
    "feats3 = reviews.copy()\n",
    "feats3[\"feats\"] = [normarray(v) for v in hvcounts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3 = append_pca_columns(feats3, \"feats\")\n",
    "alt.Chart(feats3.sample(2000)).encode(x=\"x\", y=\"y\", color=\"score\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = np.array(list(feats3[\"feats\"]))\n",
    "clustering = KMeans(n_clusters=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3[\"pred\"] = clustering.predict(np.array(list(feats3[\"feats\"])))\n",
    "feats3[\"pstr\"] = feats3[\"pred\"].apply(str)\n",
    "alt.Chart(feats3.sample(2000)).encode(x=\"x\", y=\"y\", color=\"pstr\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3[\"pdist\"] = feats3.apply(lambda row: np.linalg.norm(row[\"feats\"] - clustering.cluster_centers_[row[\"pred\"]]), axis=1)\n",
    "feats3[\"pdist\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = feats3.sort_values(by=[\"pdist\"], ascending=False)[[\"pdist\",\"score\",\"sentiment\",\"text\"]].head(25)\n",
    "showtxt(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
