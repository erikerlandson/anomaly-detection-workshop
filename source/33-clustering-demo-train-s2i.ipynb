{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection With Clustering\n",
    "\n",
    "Many data sets take the form of vector values.\n",
    "One relatively simple but very effective way of defining expected behavior for this kind of data is to cluster it, and use these clusters as a model of our expectations.\n",
    "Data samples that do not fall inside, or near, any cluster are often anomalous in some way.\n",
    "\n",
    "In this notebook we will work with\n",
    "[Amazon Fine Food Reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews/)\n",
    "data[1] from Kaggle.\n",
    "We will generate feature vectors from this data and _cluster_ them to use as an anomaly detection model.\n",
    "\n",
    "[1] J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet files used in this notebook were created from the raw Kaggle CSV as follows:\n",
    "```python\n",
    "with open(\"data/amazon-reviews.csv\") as f:\n",
    "    data = pd.read_csv(f)\n",
    "data = data.sample(10000).reset_index(drop=True)\n",
    "data = data.drop(columns=[\"Id\", \"ProductId\", \"UserId\", \"ProfileName\", \"Time\", \"Summary\"])\n",
    "data[\"hscore\"] = \\\n",
    "    data.apply(lambda row: (1+row[\"HelpfulnessNumerator\"]) / (2+row[\"HelpfulnessDenominator\"]), axis=1)\n",
    "data = data.drop(columns=[\"HelpfulnessNumerator\", \"HelpfulnessDenominator\"])\n",
    "data = data.rename(columns={\"Score\":\"score\", \"Text\":\"text\"})\n",
    "data = data[[\"score\", \"hscore\", \"text\"]]\n",
    "data.to_parquet(\"data/amazon-reviews-10K.parquet\", compression=\"brotli\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "The raw food review data has been sub-sampled to 50,000 records and stored as a parquet file to reduce its footprint on disk.\n",
    "\n",
    "We begin by loading the data.\n",
    "You can see that each review comes with a score, from one to five \"stars\", a helpfulness score, and the review text.\n",
    "In this lab we will not be using the helpfulness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_parquet(\"data/amazon-reviews-50K.parquet\").reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlbr = re.compile('<br />')\n",
    "whitesp = re.compile('\\\\s+')\n",
    "def cleantxt(txt):\n",
    "    clean = re.sub(htmlbr, ' ', txt)\n",
    "    clean = re.sub(whitesp, ' ', clean)\n",
    "    clean = clean.lower()\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies with Word Features\n",
    "\n",
    "The previous two variations on feature vectors for anomaly detection suggest that there is no one way to define what is \"anomalous\".\n",
    "What we detect as an anomaly depends on how we define our expectations.\n",
    "That in turn depends on what kind of features we collect in the first place.\n",
    "\n",
    "With that in mind, what will happen if we replace shingles with whole words for generating hashed frequency vectors?\n",
    "\n",
    "In the following cells we apply the SKLearn hashing vectorizer to create a hashed vector of word counts.\n",
    "As before, we will normalize these vectors to a length of 1 to put different review lengths on an equal footing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "HVSIZE = 100\n",
    "vectorizer = HashingVectorizer(token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', n_features = HVSIZE, alternate_sign=False)\n",
    "hvcounts = vectorizer.fit_transform(reviews[\"text\"].apply(cleantxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normarray(v):\n",
    "    r = v.toarray().reshape(HVSIZE)\n",
    "    z = np.linalg.norm(r)\n",
    "    if (z > 0.0): r /= z\n",
    "    return r\n",
    "\n",
    "reviews[\"feats\"] = [normarray(v) for v in hvcounts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(list(reviews[\"feats\"]))\n",
    "clustering = KMeans(n_clusters=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = [[\"scikit-learn\", \"0.21.3\"], [\"pandas\", \"0.23.0\"], [\"numpy\", \"1.17.0\"], [\"scipy\", \"1.3.1\"], [\"pyarrow\", \"0.15.1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(x):\n",
    "    xclean = cleantxt(x)\n",
    "    xvec = normarray(vectorizer.transform([xclean]))\n",
    "    xj = clustering.predict([xvec])[0]\n",
    "    xc = clustering.cluster_centers_[xj]\n",
    "    return np.linalg.norm(xc - xvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validator(x):\n",
    "    return isinstance(x, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = reviews[\"text\"][0]\n",
    "#predictor(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
