{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating quantiles with the t-digest\n",
    "\n",
    "The [t-digest](https://github.com/tdunning/t-digest) is a compact data structure for summarizing observed cumulative probability distributions.  Like the other structures we've discussed, it's incremental, scalable, and parallel (although the implementation in this notebook is not parallel).  Also like the other structures we've discussed, it has many useful applications in systems, performance analysis, and data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "alt.renderers.enable(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-Digest\n",
    "\n",
    "In the following cell we will import a `TDigest` class that implements the `update` operation for inserting data elements into the sketch. This tutorial implementation does not provide the `merge` operation, which is useful for combining partial results in a distributed computing setting.\n",
    "\n",
    "For those interested in how t-digests operate, the implementation is in the `tdigest.py` file of this workshop repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detail.tdigest import TDigest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketching data with the t-digest\n",
    "\n",
    "This data has a similar shape to the Poisson distribution, which means that it could resemble latencies for a network service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = 1\n",
    "mu2 = 7\n",
    "w1 = 0.5\n",
    "w2 = 0.5\n",
    "\n",
    "def distsamp():\n",
    "    r = scipy.stats.uniform.rvs(size=1)[0]\n",
    "    if (r <= w1):\n",
    "        return scipy.stats.gamma.rvs(mu1, size=1)[0]\n",
    "    else:\n",
    "        return scipy.stats.gamma.rvs(mu2, size=1)[0]\n",
    "\n",
    "sketch = TDigest(compression = 0.1)\n",
    "\n",
    "for p in [distsamp() for x in range(100000)]:\n",
    "    sketch.update(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals = np.arange(sketch.cdfi(0), sketch.cdfi(1)).tolist()\n",
    "df = pd.DataFrame()\n",
    "df[\"x\"] = xvals + xvals\n",
    "df[\"cdf\"] = [sketch.cdf(x) for x in xvals] + [(w1 * scipy.stats.gamma.cdf(x, mu1)) + (w2 * scipy.stats.gamma.cdf(x, mu2)) for x in xvals]\n",
    "df[\"src\"] = ([\"tdigest\"] * len(xvals)) + ([\"cdf\"] * len(xvals))\n",
    "alt.Chart(df).mark_line().encode(x=\"x\", y=\"cdf\", color=\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDigestAnomalyDetector(object):\n",
    "    def __init__(self, td):\n",
    "        self.td = td\n",
    "\n",
    "    # use the negative log of the tail probability as the score\n",
    "    def anomaly(self, x):\n",
    "        cdf = self.td.cdf(x)\n",
    "        # Here we'll assume we're only testing for \"large\" anomalies\n",
    "        t = 1 - cdf\n",
    "        # make sure we don't try to take the logarithm of zero\n",
    "        t = max(t, 1e-100)\n",
    "        return -math.log(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = TDigestAnomalyDetector(sketch)\n",
    "data = [0, 5, 10, 15, 20, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame({\n",
    "  'x': data,\n",
    "  'score': [detector.anomaly(x) for x in data]\n",
    "})\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(scores).mark_line(point=True,clip=True).encode(\n",
    "    alt.Y('score', scale=alt.Scale(domain=(0, 10))),\n",
    "    x='x'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
