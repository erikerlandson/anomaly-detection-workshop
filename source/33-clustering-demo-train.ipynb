{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection With Clustering\n",
    "\n",
    "Many data sets take the form of vector values.\n",
    "One relatively simple but very effective way of defining expected behavior for this kind of data is to cluster it, and use these clusters as a model of our expectations.\n",
    "Data samples that do not fall inside, or near, any cluster are often anomalous in some way.\n",
    "\n",
    "In this notebook we will work with\n",
    "[Amazon Fine Food Reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews/)\n",
    "data[1] from Kaggle.\n",
    "We will generate feature vectors from this data and _cluster_ them to use as an anomaly detection model.\n",
    "\n",
    "[1] J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet files used in this notebook were created from the raw Kaggle CSV as follows:\n",
    "```python\n",
    "with open(\"data/amazon-reviews.csv\") as f:\n",
    "    data = pd.read_csv(f)\n",
    "data = data.sample(10000).reset_index(drop=True)\n",
    "data = data.drop(columns=[\"Id\", \"ProductId\", \"UserId\", \"ProfileName\", \"Time\", \"Summary\"])\n",
    "data[\"hscore\"] = \\\n",
    "    data.apply(lambda row: (1+row[\"HelpfulnessNumerator\"]) / (2+row[\"HelpfulnessDenominator\"]), axis=1)\n",
    "data = data.drop(columns=[\"HelpfulnessNumerator\", \"HelpfulnessDenominator\"])\n",
    "data = data.rename(columns={\"Score\":\"score\", \"Text\":\"text\"})\n",
    "data = data[[\"score\", \"hscore\", \"text\"]]\n",
    "data.to_parquet(\"data/amazon-reviews-10K.parquet\", compression=\"brotli\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install altair vega pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.stats import gamma, kstest\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from detail.altairdf import altairDF\n",
    "alt.renderers.enable(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterdf(df, pred):\n",
    "    return df.loc[[idx for idx in df.index if pred(df.loc[idx])]]\n",
    "def showtxt(df, subset = [\"text\"]):\n",
    "    return df.style \\\n",
    "             .applymap(lambda x: 'white-space:wrap', subset=subset) \\\n",
    "             .applymap(lambda x:'text-align:left', subset=subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "The raw food review data has been sub-sampled to 50,000 records and stored as a parquet file to reduce its footprint on disk.\n",
    "\n",
    "We begin by loading the data.\n",
    "You can see that each review comes with a score, from one to five \"stars\", a helpfulness score, and the review text.\n",
    "In this lab we will not be using the helpfulness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_parquet(\"data/amazon-reviews-50K.parquet\").reindex()\n",
    "showtxt(reviews.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlbr = re.compile('<br />')\n",
    "whitesp = re.compile('\\\\s+')\n",
    "def cleantxt(txt):\n",
    "    clean = re.sub(htmlbr, ' ', txt)\n",
    "    clean = re.sub(whitesp, ' ', clean)\n",
    "    clean = clean.lower()\n",
    "    return clean\n",
    "\n",
    "def hashing_frequency(vecsize, h, norm = 1.0):\n",
    "    def hf(words):\n",
    "        if type(words) is type(\"\"):\n",
    "            # handle both lists of words and space-delimited strings\n",
    "            words = words.split(\" \")\n",
    "        hsig = np.zeros(vecsize, dtype=np.float32)\n",
    "        for term in [w for w in words if len(w) > 0]:\n",
    "            hsig[h(term) % vecsize] += 1.0\n",
    "        z = np.linalg.norm(hsig) / norm\n",
    "        if (z > 0.0): hsig /= z\n",
    "        return hsig\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing\n",
    "\n",
    "As before, we would like to visualize our data.\n",
    "However, in this case, our shingle-based features have hundreds of dimensions.\n",
    "So we will apply Principle Component Analysis (PCA) to project our features down to 2 dimensions and observe their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "def append_pca_columns(df, featcol, pcacols=[\"x\", \"y\"]):\n",
    "    DIMENSIONS = 2\n",
    "    data = np.array(list(df[featcol]))\n",
    "    pca2 = sklearn.decomposition.PCA(DIMENSIONS)\n",
    "    pca = pca2.fit_transform(data)\n",
    "    pca_df = pd.DataFrame(pca, columns=pcacols)\n",
    "    df = df.drop(columns=pcacols, errors='ignore')\n",
    "    df = pd.concat([df, pca_df], axis=1).reindex()\n",
    "    return df\n",
    "\n",
    "def pca_features(df, icol, ocol, dimensions=2):\n",
    "    data = np.array(list(df[icol]))\n",
    "    pca2 = sklearn.decomposition.PCA(dimensions)\n",
    "    pca = pca2.fit_transform(data)\n",
    "    df[ocol] = list(pca)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies with Word Features\n",
    "\n",
    "The previous two variations on feature vectors for anomaly detection suggest that there is no one way to define what is \"anomalous\".\n",
    "What we detect as an anomaly depends on how we define our expectations.\n",
    "That in turn depends on what kind of features we collect in the first place.\n",
    "\n",
    "With that in mind, what will happen if we replace shingles with whole words for generating hashed frequency vectors?\n",
    "\n",
    "In the following cells we apply the SKLearn hashing vectorizer to create a hashed vector of word counts.\n",
    "As before, we will normalize these vectors to a length of 1 to put different review lengths on an equal footing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "\n",
    "HVSIZE = 100\n",
    "vectorizer = HashingVectorizer(token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', n_features = HVSIZE, alternate_sign=False)\n",
    "hvcounts = vectorizer.fit_transform(reviews[\"text\"].apply(cleantxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normarray(v):\n",
    "    r = v.toarray().reshape(HVSIZE)\n",
    "    z = np.linalg.norm(r)\n",
    "    if (z > 0.0): r /= z\n",
    "    return r\n",
    "\n",
    "feats3 = reviews.copy()\n",
    "feats3[\"feats\"] = [normarray(v) for v in hvcounts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Again we use PCA get our feature vectors into a visualizable form.\n",
    "In this low dimensionality there is relatively little structure evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3 = append_pca_columns(feats3, \"feats\")\n",
    "alt.Chart(feats3.sample(2000)).encode(x=\"x\", y=\"y\", color=\"score\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Hashed Word Counts\n",
    "\n",
    "As with hashed shingles, the word-based clusters show a lot of overlap in low dimensional projections,\n",
    "but possible outliers are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = np.array(list(feats3[\"feats\"]))\n",
    "clustering = KMeans(n_clusters=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3[\"pred\"] = clustering.predict(np.array(list(feats3[\"feats\"])))\n",
    "feats3[\"pstr\"] = feats3[\"pred\"].apply(str)\n",
    "alt.Chart(feats3.sample(2000)).encode(x=\"x\", y=\"y\", color=\"pstr\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashed Word Anomalies\n",
    "\n",
    "We apply our now-familiar technique of identifying reviews which are not near any cluster, and sorting by distance.\n",
    "You can see that word-based features identify different anomalies than shingle-based features, even though they are both representations of the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3[\"pdist\"] = feats3.apply(lambda row: np.linalg.norm(row[\"feats\"] - clustering.cluster_centers_[row[\"pred\"]]), axis=1)\n",
    "feats3[\"pdist\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = feats3.sort_values(by=[\"pdist\"], ascending=False)[[\"pdist\",\"score\",\"text\"]].head(25)\n",
    "showtxt(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. How are the anomalies detected with hashed word frequencies different than hashed shingles?\n",
    "1. Can you think of explainations for this difference?\n",
    "1. Are the anomalies detected by all the different features in this notebook equally useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
