{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection With Clustering\n",
    "\n",
    "Many data sets take the form of vector values.\n",
    "One relatively simple but very effective way of defining expected behavior for this kind of data is to cluster it, and use these clusters as a model of our expectations.\n",
    "Data samples that do not fall inside, or near, any cluster are often anomalous in some way.\n",
    "\n",
    "In this notebook we will work with\n",
    "[Amazon Fine Food Reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews/)\n",
    "data[1] from Kaggle.\n",
    "We will generate feature vectors from this data and _cluster_ them to use as an anomaly detection model.\n",
    "\n",
    "[1] J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet files used in this notebook were created from the raw Kaggle CSV as follows:\n",
    "```python\n",
    "with open(\"data/amazon-reviews.csv\") as f:\n",
    "    data = pd.read_csv(f)\n",
    "data = data.sample(10000).reset_index(drop=True)\n",
    "data = data.drop(columns=[\"Id\", \"ProductId\", \"UserId\", \"ProfileName\", \"Time\", \"Summary\"])\n",
    "data[\"hscore\"] = \\\n",
    "    data.apply(lambda row: (1+row[\"HelpfulnessNumerator\"]) / (2+row[\"HelpfulnessDenominator\"]), axis=1)\n",
    "data = data.drop(columns=[\"HelpfulnessNumerator\", \"HelpfulnessDenominator\"])\n",
    "data = data.rename(columns={\"Score\":\"score\", \"Text\":\"text\"})\n",
    "data = data[[\"score\", \"hscore\", \"text\"]]\n",
    "data.to_parquet(\"data/amazon-reviews-10K.parquet\", compression=\"brotli\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment altair vega pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.stats import gamma, kstest\n",
    "import pandas as pd\n",
    "import vaderSentiment.vaderSentiment as vader\n",
    "from sklearn.cluster import KMeans\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from detail.altairdf import altairDF\n",
    "alt.renderers.enable(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterdf(df, pred):\n",
    "    return df.loc[[idx for idx in df.index if pred(df.loc[idx])]]\n",
    "def showtxt(df, subset = [\"text\"]):\n",
    "    return df.style \\\n",
    "             .applymap(lambda x: 'white-space:wrap', subset=subset) \\\n",
    "             .applymap(lambda x:'text-align:left', subset=subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data\n",
    "\n",
    "The raw food review data has been sub-sampled to 50,000 records and stored as a parquet file to reduce its footprint on disk.\n",
    "\n",
    "We begin by loading the data.\n",
    "You can see that each review comes with a score, from one to five \"stars\", a helpfulness score, and the review text.\n",
    "In this lab we will not be using the helpfulness score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_parquet(\"data/amazon-reviews-50K.parquet\").reindex()\n",
    "showtxt(reviews.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Each reviewer supplied a rating from 1 to 5 stars.\n",
    "One interesting question we might ask is:\n",
    "does a natural language processing method like sentiment analysis of the reviews themselves agree with this rating?\n",
    "\n",
    "In this lab we'll use the\n",
    "[vader sentiment analysis library](https://github.com/cjhutto/vaderSentiment).\n",
    "The following cell defines a function that breaks each review into sentences and extracts vader's numeric assessment of positive or negative sentiment.\n",
    "The most positive sentiment is 1.0 and the most negative sentiment is -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english = spacy.load('en_core_web_sm')\n",
    "#def sentences(text):\n",
    "#    return [str(s) for s in english(text).sents]\n",
    "\n",
    "sdelim = re.compile('(?<=[.!?]) *')\n",
    "def sentences(text):\n",
    "    return [s for s in re.split(sdelim, text) if len(s) > 1]\n",
    "\n",
    "sentiment = vader.SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_compound(text):\n",
    "    scores = [sentiment.polarity_scores(s)['compound'] for s in sentences(text)]\n",
    "    if len(scores) < 1: return 0.0\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reviews[\"sentiment\"] = reviews[\"text\"].apply(sentiment_compound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Feature Vector\n",
    "\n",
    "We'll begin by assembling a simple feature vector, with only two dimensions: the review score, and a corresponding sentiment value taken from vader.\n",
    "Many clustering algorithms perform best when individual feature dimensions occupy roughly the same scale, and so we will _normalize_ the review scores to be from 0.0 to 1.0 instead of 1 to 5.\n",
    "In the following we assemble these \"2-vectors\" as numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "feats1 = reviews.copy().reindex()\n",
    "feats1[\"feats\"] = feats1.apply(lambda row: np.array([row[\"score\"] / 5.0, row[\"sentiment\"]]), axis=1)\n",
    "feats1[\"feats\"].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Features\n",
    "\n",
    "It is useful to _visualize_ your data.\n",
    "Since our features have only two dimensions (review score and sentiment), we can directly view our points in these dimensions with a scatter plot.\n",
    "The following plot illustrates that the review scores exhibit only five values but the corresponding sentiments have much more variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1[\"x: score\"] = feats1[\"feats\"].apply(lambda x: x[0])\n",
    "feats1[\"y: sentiment\"] = feats1[\"feats\"].apply(lambda x: x[1])\n",
    "alt.Chart(feats1.sample(2000)).encode(x=\"x: score\", y=\"y: sentiment\", color=\"score\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "To build our model of expected food reviews, we will apply k-means clustering.\n",
    "K-Means clustering is one of the oldest clustering methods.\n",
    "It is relatively fast, and can be parallelized either via hardware accelerators or scale-out platforms such as Dask or Spark.\n",
    "\n",
    "In the following cells we apply sklearn's k-means clustering, and overlay the clustering on a scatter plot of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = np.array(list(feats1[\"feats\"]))\n",
    "clustering = KMeans(n_clusters=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1[\"pred\"] = clustering.predict(np.array(list(feats1[\"feats\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1[\"pstr\"] = feats1[\"pred\"].apply(str)\n",
    "alt.Chart(feats1.sample(2000)).encode(x=\"x: score\", y=\"y: sentiment\", color=\"pstr\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatterplot above, you can see that the clusters do not necessarily align with how a human might place them.\n",
    "We will see that, at least for anomaly detection, our results are not very sensitive to the particular locations of our clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies\n",
    "\n",
    "In the following cells, we use our clustering to identify some anomalies in our data.\n",
    "For each review, we compute its distance to the nearest cluster.\n",
    "Reviews that are not near any cluster are candidate anomalies.\n",
    "We will sort them so that largest distances are at the top, and look at the first few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1[\"pdist\"] = feats1.apply(lambda row: np.linalg.norm(row[\"feats\"] - clustering.cluster_centers_[row[\"pred\"]]), axis=1)\n",
    "feats1[\"pdist\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = feats1.sort_values(by=[\"pdist\"], ascending=False)[[\"pdist\",\"sentiment\",\"score\",\"text\"]].head(25)\n",
    "showtxt(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. In the anomaly results above, there are cases of a positive review (4 or 5) but negative sentiment. How would you interpret this kind of anomaly?\n",
    "1. You may also see examples of negative review (1 or 2) but positive sentiment. What would you conclude from this?\n",
    "1. Some samples may show total agreement between sentiment and anomaly. What is a possible explanation of these  \"false positive\" anomalies?\n",
    "1. Can you think of a different way to detect these score/sentiment anomalies that does not require clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies from Text\n",
    "\n",
    "In the previous section we looked at anomalies by clustering two numeric features.\n",
    "Next we will look at a method for clustering based on the review text, using basic concepts from natural language processing (NLP).\n",
    "\n",
    "In the following cell, we define some functions for extracting shingles (aka\n",
    "[n-grams](https://en.wikipedia.org/wiki/N-gram)),\n",
    "along with some simple logic for cleaning the raw text up.\n",
    "\n",
    "This simple example illustrates 3-shingles from some text:\n",
    "```\n",
    "shingles(\"dog tail\", 3)\n",
    "=>\n",
    "\"dog\"\n",
    " \"og \"\n",
    "  \"g t\"\n",
    "   \" ta\"\n",
    "    \"tai\"\n",
    "     \"ail\"\n",
    "```\n",
    "\n",
    "The number of possible shingles is quite large, particularly for shingles of size 3 or higher.\n",
    "To work with this, we will _hash_ our shingles into a fixed-length feature vector that contains the count of each shingle in the review text.\n",
    "We'll also normalize these vectors so that they are all of length 1, to compensate for texts of differing lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingles(k):\n",
    "    def kshingles(doc):\n",
    "        return [doc[i:i + k] for i in range(len(doc) - k + 1)]\n",
    "    return kshingles\n",
    "\n",
    "htmlbr = re.compile('<br />')\n",
    "whitesp = re.compile('\\\\s+')\n",
    "def cleantxt(txt):\n",
    "    clean = re.sub(htmlbr, ' ', txt)\n",
    "    clean = re.sub(whitesp, ' ', clean)\n",
    "    clean = clean.lower()\n",
    "    return clean\n",
    "\n",
    "def hashing_frequency(vecsize, h, norm = 1.0):\n",
    "    def hf(words):\n",
    "        if type(words) is type(\"\"):\n",
    "            # handle both lists of words and space-delimited strings\n",
    "            words = words.split(\" \")\n",
    "        hsig = np.zeros(vecsize, dtype=np.float32)\n",
    "        for term in [w for w in words if len(w) > 0]:\n",
    "            hsig[h(term) % vecsize] += 1.0\n",
    "        z = np.linalg.norm(hsig) / norm\n",
    "        if (z > 0.0): hsig /= z\n",
    "        return hsig\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sh4 = shingles(4)\n",
    "hsig = hashing_frequency(512, hash, norm = 1)\n",
    "feats2 = reviews.copy()\n",
    "feats2[\"feats\"] = feats2[\"text\"].apply(lambda txt: hsig(sh4(cleantxt(txt))))\n",
    "feats2[\"feats\"].sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing\n",
    "\n",
    "As before, we would like to visualize our data.\n",
    "However, in this case, our shingle-based features have hundreds of dimensions.\n",
    "So we will apply Principle Component Analysis (PCA) to project our features down to 2 dimensions and observe their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "def append_pca_columns(df, featcol, pcacols=[\"x\", \"y\"]):\n",
    "    DIMENSIONS = 2\n",
    "    data = np.array(list(df[featcol]))\n",
    "    pca2 = sklearn.decomposition.PCA(DIMENSIONS)\n",
    "    pca = pca2.fit_transform(data)\n",
    "    pca_df = pd.DataFrame(pca, columns=pcacols)\n",
    "    df = df.drop(columns=pcacols, errors='ignore')\n",
    "    df = pd.concat([df, pca_df], axis=1).reindex()\n",
    "    return df\n",
    "\n",
    "def pca_features(df, icol, ocol, dimensions=2):\n",
    "    data = np.array(list(df[icol]))\n",
    "    pca2 = sklearn.decomposition.PCA(dimensions)\n",
    "    pca = pca2.fit_transform(data)\n",
    "    df[ocol] = list(pca)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats2 = append_pca_columns(feats2, \"feats\")\n",
    "alt.Chart(feats2.sample(2000)).encode(x=\"x\", y=\"y\", color=\"score\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "As before, we will apply k-means clustering to our new text shingle features.\n",
    "When we plot the clusters in 2D, there is less separation due to their locations in higher dimensional space being collapsed to 2.\n",
    "However, we can see some points that look like possible outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = np.array(list(feats2[\"feats\"]))\n",
    "clustering = KMeans(n_clusters=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats2[\"pred\"] = clustering.predict(np.array(list(feats2[\"feats\"])))\n",
    "feats2[\"pstr\"] = feats2[\"pred\"].apply(str)\n",
    "alt.Chart(feats2.sample(2000)).encode(x=\"x\", y=\"y\", color=\"pstr\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies with Text Shingles\n",
    "\n",
    "Also as before, we can map reviews to their smallest distance to a cluster and sort so the reviews farthest from any cluster are first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats2[\"pdist\"] = feats2.apply(lambda row: np.linalg.norm(row[\"feats\"] - clustering.cluster_centers_[row[\"pred\"]]), axis=1)\n",
    "feats2[\"pdist\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anomalies = feats2.sort_values(by=[\"pdist\"], ascending=False)[[\"pdist\",\"score\",\"sentiment\",\"text\"]].head(25)\n",
    "showtxt(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. How are the anomalies shown above different than the score/sentiment anomalies?\n",
    "1. What do these anomalies have in common? How does that relate to features that we collected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomalies with Word Features\n",
    "\n",
    "The previous two variations on feature vectors for anomaly detection suggest that there is no one way to define what is \"anomalous\".\n",
    "What we detect as an anomaly depends on how we define our expectations.\n",
    "That in turn depends on what kind of features we collect in the first place.\n",
    "\n",
    "With that in mind, what will happen if we replace shingles with whole words for generating hashed frequency vectors?\n",
    "\n",
    "In the following cells we apply the SKLearn hashing vectorizer to create a hashed vector of word counts.\n",
    "As before, we will normalize these vectors to a length of 1 to put different review lengths on an equal footing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "\n",
    "HVSIZE = 1000\n",
    "vectorizer = HashingVectorizer(token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', n_features = HVSIZE, alternate_sign=False)\n",
    "hvcounts = vectorizer.fit_transform(reviews[\"text\"].apply(cleantxt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normarray(v):\n",
    "    r = v.toarray().reshape(HVSIZE)\n",
    "    z = np.linalg.norm(r)\n",
    "    if (z > 0.0): r /= z\n",
    "    return r\n",
    "\n",
    "feats3 = reviews.copy()\n",
    "feats3[\"feats\"] = [normarray(v) for v in hvcounts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Again we use PCA get our feature vectors into a visualizable form.\n",
    "In this low dimensionality there is relatively little structure evident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3 = append_pca_columns(feats3, \"feats\")\n",
    "alt.Chart(feats3.sample(2000)).encode(x=\"x\", y=\"y\", color=\"score\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Hashed Word Counts\n",
    "\n",
    "As with hashed shingles, the word-based clusters show a lot of overlap in low dimensional projections,\n",
    "but possible outliers are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = np.array(list(feats3[\"feats\"]))\n",
    "clustering = KMeans(n_clusters=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3[\"pred\"] = clustering.predict(np.array(list(feats3[\"feats\"])))\n",
    "feats3[\"pstr\"] = feats3[\"pred\"].apply(str)\n",
    "alt.Chart(feats3.sample(2000)).encode(x=\"x\", y=\"y\", color=\"pstr\").mark_point().interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashed Word Anomalies\n",
    "\n",
    "We apply our now-familiar technique of identifying reviews which are not near any cluster, and sorting by distance.\n",
    "You can see that word-based features identify different anomalies than shingle-based features, even though they are both representations of the review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats3[\"pdist\"] = feats3.apply(lambda row: np.linalg.norm(row[\"feats\"] - clustering.cluster_centers_[row[\"pred\"]]), axis=1)\n",
    "feats3[\"pdist\"].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = feats3.sort_values(by=[\"pdist\"], ascending=False)[[\"pdist\",\"score\",\"sentiment\",\"text\"]].head(25)\n",
    "showtxt(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. How are the anomalies detected with hashed word frequencies different than hashed shingles?\n",
    "1. Can you think of explainations for this difference?\n",
    "1. Are the anomalies detected by all the different features in this notebook equally useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
